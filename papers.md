---
layout: page
title: Papers
---

## Papers

  * [Automated Generation of Robotic Planning Domains from Observations](https://arxiv.org/pdf/2105.13604.pdf) - IROS 2021
  * [Sim-to-Real Task Planning and Execution from Perception via Reactivity and Recovery](https://arxiv.org/abs/2011.08694) ([video](https://youtu.be/qbCzYgAW86w)) ([experiments](https://www.youtube.com/playlist?list=PL-oD0xHUngeLfQmpngYkGFZarstfPOXqX)) - IROS 2021
  * [NeRP: Neural Rearrangement Planning for Unknown Objects](https://arxiv.org/pdf/2106.01352) - RSS 2021
  * [Reactive Human-to-Robot Handovers of Arbitrary Objects](https://arxiv.org/abs/2011.08961) ([video](https://youtu.be/ZfibF9UNCrw)) ([website](https://arxiv.org/abs/2011.08961)) - ICRA 2021 Best HRI Paper *winner*
  * [Alternate Paths Planner (APP) for Provably Fixed-time Manipulation Planning in Semi-Structured Environments](https://arxiv.org/abs/2012.14970) - ICRA 2021
  * ["Good Robot!": Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165109) ([video](https://www.youtube.com/watch?v=qivDFfPf9_I)) - IROS+RAL 2020
  * [Human Grasp Classification for Reactive Human-to-Robot Handovers](https://arxiv.org/pdf/2003.06000) - IROS 2020
  * [Transferable Task Execution from Pixels through Deep Planning Domain Learning](https://arxiv.org/pdf/2003.03726) - ICRA 2020
  * [6-DOF Grasping for Target-driven Object Manipulation in Clutter](https://arxiv.org/pdf/1912.03628) - ICRA 2020
  * [Motion Reasoning for Goal-Based Imitation Learning](https://arxiv.org/pdf/1911.05864) - ICRA 2020
  * [Online Replanning in Belief Space for Partially Observable Task and Motion Problems](https://arxiv.org/pdf/1911.04577) - ICRA 2020
  * [Conditional Driving from Natural Language Insturctions](https://arxiv.org/pdf/1910.07615) - CoRL 2019
  * [Representing Robot Task Plans as Robust Logical-Dynamical Systems](https://arxiv.org/pdf/1908.01896) - IROS 2019
  * [The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints](https://arxiv.org/pdf/1810.11714.pdf) - IROS 2019
  * [Visual Robot Task Planning](https://arxiv.org/pdf/1804.00062) - ICRA 2019
  * [Evaluating Methods for End-User Creation of Robot Task Plans](https://arxiv.org/pdf/1811.02690) - IROS 2018
  * [Combining neural networks and tree search for task and motion planning in challenging environments](https://arxiv.org/pdf/1703.07887) - IROS 2017
  * [CoSTAR in Surgery: A Cross-platform User Interface for Surgical Robot Task Specification](https://pdfs.semanticscholar.org/b853/81226292ed8a47cb4e059ced14ddcc6ea798.pdf) - IROS 2017 workshop
  * [CoSTAR: Instructing Collaborative Robots through Behavior Trees and Vision](https://arxiv.org/pdf/1611.06145) - ICRA 2017
  * [Do What I Want, Not What I Did: Imitation of Skills by Planning Sequences of Actions](https://arxiv.org/pdf/1612.01215) - IROS 2016
  * [Semi-autonomous telerobotic assembly over high-latency networks](https://dl.acm.org/ft_gateway.cfm?ftid=1702218&id=2906858) - HRI 2016
  * [Towards Robot Task Planning From Probabilistic Models of Human Skills](https://arxiv.org/pdf/1602.04754) - AAAI 2016 workshop
  * [An incremental approach to learning generalizable robot tasks from human demonstration](http://eprints.lincoln.ac.uk/34493/1/ICRA15_0728_FI.pdf) - ICRA 2015
  * [A framework for end-user instruction of a robot assistant for manufacturing](https://ieeexplore.ieee.org/iel7/7128761/7138973/07140065.pdf?casa_token=mrp6oZcPvy4AAAAA:ux_-Jq6IHKCmjodjywSvdBiQyHcoeQrd-M45MfuPOTVdZxjGHEMuS3YbqLeO2Kh2XCTRl8r4xBCO) - ICRA 2015 - Seattle, USA
  * [Developing predictive models using electronic medical records: challenges and pitfalls](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900132/) - AMIA

## Notes on Selected Work

### Prospection: Interpretable Plans from Language by Predicting the Future

High-level human instructions often correspond to behaviors with multiple implicit steps. In order for robots to be useful in the real world, they must be able to to reason over both motions and intermediate goals implied by human instructions. In this work, we propose a framework for learning representations that convert from a natural-language command to a sequence of intermediate goals for execution on a robot. A key feature of this framework is prospection, training an agent not just to correctly execute the prescribed command, but to predict a horizon of consequences of an action before taking it. We demonstrate the fidelity of plans generated by our framework when interpreting real, crowd-sourced natural language commands for a robot in simulated scenes.

```
@article{paxton2019prospection,
  author    = {Chris Paxton and
               Yonatan Bisk and
               Jesse Thomason and
               Arunkumar Byravan and
               Dieter Fox},
  title     = {Prospection: Interpretable Plans From Language By Predicting the Future},
  journal   = {International Conference on Robotics and Automation (ICRA), 2019 IEEE/RSJ International Conference on},
  year      = {2019}
  url       = {http://arxiv.org/abs/1903.08309},
}
```

Presented at ICRA 2019 in Montreal. Link: [Prospection](https://arxiv.org/abs/1903.08309)

### Visual Robot Task Planning

We propose an approach that allows us to [visualize intermediate goals](https://cpaxton.github.io/2018/03/30/vtp.html) and learn to plan complex activity from visual information.

```
@article{paxton2019visual,
  author    = {Chris Paxton and
               Yotam Barnoy and
               Kapil D. Katyal and
               Raman Arora and
               Gregory D. Hager},
  title     = {Visual Robot Task Planning},
  journal   = {International Conference on Robotics and Automation (ICRA), 2019 IEEE/RSJ International Conference on},
  year      = {2019},
  url       = {http://arxiv.org/abs/1804.00062},
}
```

Presented at ICRA 2019 in Montreal.  Link: [Visual Robot Task Planning](https://arxiv.org/abs/1804.00062)


### Evaluating Methods for End-User Creation of Robot Task Plans

How can we enable users to create effective, perception-driven task plans for collaborative robots? We conducted a 35-person user study with CoSTAR to [find out](https://cpaxton.github.io/2018/08/14/evaluating.html).

```
@article{paxton2018evaluating,
  author    = {Chris Paxton and
               Felix Jonathan and
               Andrew Hundt and
               Bilge Mutlu and
               Gregory D. Hager},
  title     = {Evaluating Methods for End-User Creation of Robot Task Plans},
  journal   = {Intelligent Robots and Systems (IROS), 2018 IEEE/RSJ International Conference on},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.02690},
}
```

### Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments

Tree search based task and motion planning, combining multiple low-level policies learned via deep reinforcement learning and using them to create high-level task plans that navigate through intersections.

```
@article{paxton2017combining,
  title={Combining neural networks and tree search for task and motion planning in challenging environments},
  author={Paxton, Chris and Raman, Vasumathi and Hager, Gregory D and Kobilarov, Marin},
  journal={Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on},
  note={Available as arXiv preprint arXiv:1703.07887},
  year={2017}
}
```
Presented at IROS 2017 in Vancouver, Canada, as well as a poster presentation at RLDM in Ann Arbor, Michigan, USA. This work was partially supported by Zoox, Inc.

Arxiv link: [Neural nets and tree search](https://arxiv.org/abs/1703.07887)

### CoSTAR: Instructing Collaborative Robots with Behavior Trees and Vision

Our new and improved CoSTAR system, with 3D pose recognition. This paper describes how we built a cross-platform system for authoring complex robot task plans with behavior trees. Winner of the KUKA innovation award.

Citation:
```
@article{paxton2017costar,
  title={Co{STAR}: Instructing Collaborative Robots with Behavior Trees and Vision},
  author={Paxton, Chris and Hundt, Andrew and Jonathan, Felix and Guerin, Kelleher and Hager, Gregory D},
  journal={Robotics and Automation (ICRA), 2017 IEEE International Conference on},
  note={Available as arXiv preprint arXiv:1611.06145},
  year={2017}
}
```
Presented at ICRA 2017 in Singapore.

### Do What I Want, Not What I Did: Imitation of Skills by Planning Sequences of Actions

Sampling-based task and motion planning using skills learned from expert demonstrations.

Citation:
```
@inproceedings{paxton2016want,
  title={Do what I want, not what I did: Imitation of skills by planning sequences of actions},
  author={Paxton, Chris and Jonathan, Felix and Kobilarov, Marin and Hager, Gregory D},
  booktitle={Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on},
  pages={3778--3785},
  year={2016},
  organization={IEEE}
}
```

Presented at IROS 2016 in Daejeon, Korea.

## Other Robotics Papers

  * ***Paxton, C.***, Bisk, Y., Thomason, J., Byravan, A., and Fox, D. (2019) _Prospection: Interpretable Plans from Language by Predicting the Future_. At IEEE International Conference on Robotics and Automation (ICRA 2019). [arXiv](https://arxiv.org/abs/1903.08309).
  
  * ***Paxton, C.,*** Barnoy, Y., Katyal, K., Arora, R., and Hager, G.D. (2019) _Visual Robot Task Planning_. At IEEE Conference on Robotics and Automation (ICRA 2019). [arXiv](https://arxiv.org/abs/1804.00062)
  
  * Katyal, K., Popek, K., ***Paxton, C.***, Burlina, P., and Hager, G.D. (2019) _Uncertainty-Aware Occupancy Map Prediction Using Generative Networks for Robot Navigation_. At IEEE Conference on Robotics and Automation (ICRA 2019).
  
  * ***Paxton, C.***, Jonathan, F., Hundt, A., Mutlu, B., and Hager, G.D. (2018) _Evaluating Methods for End-User Creation of Robot Task Plans_. At IEEE Conference on Intelligent Robots and Systems (IROS 2018). [arXiv](https://arxiv.org/abs/1811.02690) [video](https://youtu.be/uxfKluW-OWI) 

  * ***Paxton, C.***, Raman, V., Hager, G.D., and Kobilarov, M. (2017) _Combining Neural Nets and Tree Search for Task and Motion Planning in Complex Environments_. At IEEE Conference on Intelligent Robots and Systems (IROS 2017). Also presented at Reinforcement Learning and Decision Making (RLDM 2017). [arXiv](https://arxiv.org/abs/1703.07887) [video](https://youtu.be/MM2U_SGMtk8) 
  
  * ***Paxton, C.***, Hundt, A., Jonathan, F., Guerin, K., and Hager, G.D. (2017) CoSTAR: Instructing Collaborative Robots with Behavior Trees and Vision. At IEEE Conference on Robotics and Automation (ICRA 2017). [arXiv](https://arxiv.org/abs/1611.06145) [video](https://youtu.be/eGdwl1dmTrA)

  * ***Paxton, C.***, Jonathan, F., Kobilarov, M., and Hager, G.D. (2016) _Do What I Want, Not What I Did: Imitation of Skills by Planning Sequences of Actions_. At IEEE Conference on Intelligent Robots and Systems (IROS 2016). [arXiv](https://arxiv.org/abs/1612.01215) [video 1](https://youtu.be/kESahDyedR4) [video 2](https://youtu.be/MaIb6K--yv0)
  
  * Bohren, J., ***Paxton, C.***, Howarth, R., Hager, G.D., and Whitcomb, L. (2016). _Enabling Semi-Autonomous Teleoperated Assembly over High-Latency Networks_. At 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI 2016). Nominated for best technical contribution to human-robot interaction.

  * Ghalamzan E., A. M., ***Paxton, C.***, Hager, G.D., & Bascetta, L. (2015). _An Incremental Approach to Learning Generalizable Robot Tasks from Human Demonstration_. In IEEE International Conference on Robotics and Automation (ICRA).

  * Guerin, K., Lea, C., ***Paxton, C.***, & Hager, G.D. (2015). _A Framework for End-User Instruction of a Robot Assistant for Manufacturing_. In IEEE International Conference on Robotics and Automation (ICRA).

## Workshop Papers

  * ***Paxton, C.***, Kobilarov, M., & Hager, G.D. (2015). _Towards Robot Task Planning from Probabilistic Representations of Human Skills_. At AAAI 2016 Workshop on Planning for Hybrid Systems. [pdf]({{ site.baseurl}}public/paxton2016towards-planhs.pdf) [arXiv](http://arxiv.org/abs/1602.04754)

  * Ghalamzan E., A. M., ***Paxton, C.***, Hager, G.D., & Bascetta, L. (2014). _Learning How to Avoid an Obstacle from Human Demonstration_. At RSS 2014 workshop on Learning Plans with Context from Human Signals.

  * ***Paxton, C.***, Bohren, J., and Hager, G. D. (2014). _Standards for Grounding Symbols for Robotic Task Ontologies_. At IROS 2014 workshop on Standardized Knowledge Representation and Ontologies for Robotics and Automation. 

## Previous Work

<p class="message">
When working on my Masters, I did some research in early prediction of sepsis before switching directions completely and starting to work on learning task representations for robot motion planning.
</p>

  * Henry, K., ***Paxton, C.***, Kim, K.S., Pham, J., & Saria, S. (2014). _REWS: Real-time Early Warning Score for Septic Shock_. In Critical Care Medicine (Vol. 42, Issue 12, p. A1384). Society for Critical Care Medicine. Winner of Annual Scientific Award. 

  * ***Paxton, C.***, Niculescu-Mizil, A., & Saria, S. (2013). _Developing Predictive Models Using Electronic Medical Records: Challenges and Pitfalls_. In AMIA Annual Symposium Proceedings (Vol. 2013, p. 1109). American Medical Informatics Association.

  * Vedula, S.S., Malpani, A.O., Tao, L., Chen, G., Gao, Y., Poddar, P., Ahmidi, N., ***Paxton, C.***, Vidal, R., Khudanpur, S., Hager, G.D., and Chen, C.C.G. (2016). _Analysis of the Structure of Surgical Activity for a Suturing and Knot-Tying Task_. PLoS ONE 11(3): e0149174. doi:10.1371/journal.pone.0149174

